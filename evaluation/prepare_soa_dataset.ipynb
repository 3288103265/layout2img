{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_xx_xx. 格式：  [{'image_id': XX, 'id': XX, 'bbox': [x0,y0,w,h]}, ...]\n",
    "# refactor datasets:\n",
    "\n",
    "import json, os, random, math\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.serialization import save\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from skimage.transform import resize as imresize\n",
    "import pycocotools.mask as mask_utils\n",
    "from random import shuffle\n",
    "# import io\n",
    "from skimage import io, img_as_ubyte\n",
    "\n",
    "\n",
    "class CocoSceneGraphDataset(Dataset):\n",
    "    def __init__(self, image_dir, instances_json, stuff_json=None,\n",
    "                 stuff_only=True, image_size=(64, 64), mask_size=16,\n",
    "                 normalize_images=True, max_samples=None,\n",
    "                 include_relationships=True, min_object_size=0.02,\n",
    "                 min_objects_per_image=3, max_objects_per_image=8, left_right_flip=False,\n",
    "                 include_other=False, instance_whitelist=None, stuff_whitelist=None):\n",
    "\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        if stuff_only and stuff_json is None:\n",
    "            print('WARNING: Got stuff_only=True but stuff_json=None.')\n",
    "            print('Falling back to stuff_only=False.')\n",
    "\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_size = mask_size\n",
    "        self.max_samples = max_samples\n",
    "        self.max_objects_per_image = max_objects_per_image\n",
    "        self.normalize_images = normalize_images\n",
    "        self.include_relationships = include_relationships\n",
    "        self.left_right_flip = left_right_flip\n",
    "        self.set_image_size(image_size)\n",
    "\n",
    "        with open(instances_json, 'r') as f:\n",
    "            instances_data = json.load(f)\n",
    "\n",
    "        stuff_data = None\n",
    "        if stuff_json is not None and stuff_json != '':\n",
    "            with open(stuff_json, 'r') as f:\n",
    "                stuff_data = json.load(f)\n",
    "\n",
    "        self.image_ids = []\n",
    "        self.image_id_to_filename = {}\n",
    "        self.image_id_to_size = {}\n",
    "        for image_data in instances_data['images']:\n",
    "            image_id = image_data['id']\n",
    "            filename = image_data['file_name']\n",
    "            width = image_data['width']\n",
    "            height = image_data['height']\n",
    "            self.image_ids.append(image_id)\n",
    "            self.image_id_to_filename[image_id] = filename\n",
    "            self.image_id_to_size[image_id] = (width, height)\n",
    "\n",
    "        self.vocab = {\n",
    "            'object_name_to_idx': {},\n",
    "            'pred_name_to_idx': {},\n",
    "        }\n",
    "        object_idx_to_name = {}\n",
    "        all_instance_categories = []\n",
    "        for category_data in instances_data['categories']:\n",
    "            category_id = category_data['id']\n",
    "            category_name = category_data['name']\n",
    "            all_instance_categories.append(category_name)\n",
    "            object_idx_to_name[category_id] = category_name\n",
    "            self.vocab['object_name_to_idx'][category_name] = category_id\n",
    "        all_stuff_categories = []\n",
    "        if stuff_data:\n",
    "            for category_data in stuff_data['categories']:\n",
    "                category_name = category_data['name']\n",
    "                category_id = category_data['id']\n",
    "                all_stuff_categories.append(category_name)\n",
    "                object_idx_to_name[category_id] = category_name\n",
    "                self.vocab['object_name_to_idx'][category_name] = category_id\n",
    "\n",
    "        if instance_whitelist is None:\n",
    "            instance_whitelist = all_instance_categories\n",
    "        if stuff_whitelist is None:\n",
    "            stuff_whitelist = all_stuff_categories\n",
    "        category_whitelist = set(instance_whitelist) | set(stuff_whitelist)\n",
    "\n",
    "        # Add object data from instances\n",
    "        self.image_id_to_objects = defaultdict(list)\n",
    "        for object_data in instances_data['annotations']:\n",
    "            image_id = object_data['image_id']\n",
    "            _, _, w, h = object_data['bbox']\n",
    "            W, H = self.image_id_to_size[image_id]\n",
    "            box_area = (w * h) / (W * H)\n",
    "            # box_area = object_data['area'] / (W * H)\n",
    "            box_ok = box_area > min_object_size\n",
    "            object_name = object_idx_to_name[object_data['category_id']]\n",
    "            category_ok = object_name in category_whitelist\n",
    "            other_ok = object_name != 'other' or include_other\n",
    "            if box_ok and category_ok and other_ok and (object_data['iscrowd'] != 1):\n",
    "                self.image_id_to_objects[image_id].append(object_data)\n",
    "\n",
    "        # Add object data from stuff\n",
    "        if stuff_data:\n",
    "            image_ids_with_stuff = set()\n",
    "            for object_data in stuff_data['annotations']:\n",
    "                image_id = object_data['image_id']\n",
    "                image_ids_with_stuff.add(image_id)\n",
    "                _, _, w, h = object_data['bbox']\n",
    "                W, H = self.image_id_to_size[image_id]\n",
    "                box_area = (w * h) / (W * H)\n",
    "                # box_area = object_data['area'] / (W * H)\n",
    "                box_ok = box_area > min_object_size\n",
    "                object_name = object_idx_to_name[object_data['category_id']]\n",
    "                category_ok = object_name in category_whitelist\n",
    "                other_ok = object_name != 'other' or include_other\n",
    "                if box_ok and category_ok and other_ok and (object_data['iscrowd'] != 1):\n",
    "                    self.image_id_to_objects[image_id].append(object_data)\n",
    "\n",
    "            if stuff_only:\n",
    "                new_image_ids = []\n",
    "                for image_id in self.image_ids:\n",
    "                    if image_id in image_ids_with_stuff:\n",
    "                        new_image_ids.append(image_id)\n",
    "                self.image_ids = new_image_ids\n",
    "\n",
    "                all_image_ids = set(self.image_id_to_filename.keys())\n",
    "                image_ids_to_remove = all_image_ids - image_ids_with_stuff\n",
    "                for image_id in image_ids_to_remove:\n",
    "                    self.image_id_to_filename.pop(image_id, None)\n",
    "                    self.image_id_to_size.pop(image_id, None)\n",
    "                    self.image_id_to_objects.pop(image_id, None)\n",
    "\n",
    "        # COCO category labels start at 1, so use 0 for __image__\n",
    "        self.vocab['object_name_to_idx']['__image__'] = 0\n",
    "\n",
    "        # Build object_idx_to_name\n",
    "        name_to_idx = self.vocab['object_name_to_idx']\n",
    "        assert len(name_to_idx) == len(set(name_to_idx.values()))\n",
    "        max_object_idx = max(name_to_idx.values())\n",
    "        idx_to_name = ['NONE'] * (1 + max_object_idx)\n",
    "        for name, idx in self.vocab['object_name_to_idx'].items():\n",
    "            idx_to_name[idx] = name\n",
    "        self.vocab['object_idx_to_name'] = idx_to_name\n",
    "\n",
    "        # Prune images that have too few or too many objects\n",
    "        new_image_ids = []\n",
    "        total_objs = 0\n",
    "        for image_id in self.image_ids:\n",
    "            num_objs = len(self.image_id_to_objects[image_id])\n",
    "            total_objs += num_objs\n",
    "            if min_objects_per_image <= num_objs <= max_objects_per_image:\n",
    "                new_image_ids.append(image_id)\n",
    "        self.image_ids = new_image_ids\n",
    "\n",
    "        self.vocab['pred_idx_to_name'] = [\n",
    "            '__in_image__',\n",
    "            'left of',\n",
    "            'right of',\n",
    "            'above',\n",
    "            'below',\n",
    "            'inside',\n",
    "            'surrounding',\n",
    "        ]\n",
    "        self.vocab['pred_name_to_idx'] = {}\n",
    "        for idx, name in enumerate(self.vocab['pred_idx_to_name']):\n",
    "            self.vocab['pred_name_to_idx'][name] = idx\n",
    "\n",
    "    def set_image_size(self, image_size):\n",
    "        print('called set_image_size', image_size)\n",
    "        transform = [Resize(image_size), T.ToTensor()]\n",
    "        if self.normalize_images:\n",
    "            transform.append(imagenet_preprocess())\n",
    "        self.transform = T.Compose(transform)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def total_objects(self):\n",
    "        total_objs = 0\n",
    "        for i, image_id in enumerate(self.image_ids):\n",
    "            if self.max_samples and i >= self.max_samples:\n",
    "                break\n",
    "            num_objs = len(self.image_id_to_objects[image_id])\n",
    "            total_objs += num_objs\n",
    "        return total_objs\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.max_samples is None:\n",
    "            if self.left_right_flip:\n",
    "                return len(self.image_ids)*2\n",
    "            return len(self.image_ids)\n",
    "        return min(len(self.image_ids), self.max_samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        flip = False\n",
    "        if index >= len(self.image_ids):\n",
    "            index = index - len(self.image_ids)\n",
    "            flip = True\n",
    "        image_id = self.image_ids[index]\n",
    "\n",
    "        filename = self.image_id_to_filename[image_id]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        with open(image_path, 'rb') as f:\n",
    "            with PIL.Image.open(f) as image:\n",
    "                if flip:\n",
    "                    image = PIL.ImageOps.mirror(image)\n",
    "                WW, HH = image.size\n",
    "                image = self.transform(image.convert('RGB'))\n",
    "\n",
    "        objs, boxes, masks = [], [], []\n",
    "        # obj_masks = []\n",
    "        for object_data in self.image_id_to_objects[image_id]:\n",
    "            objs.append(object_data['category_id'])\n",
    "            # print(self.vocab['object_idx_to_name'][object_data['category_id']])\n",
    "            x, y, w, h = object_data['bbox']\n",
    "            x0 = x / WW\n",
    "            y0 = y / HH\n",
    "            w = (w) / WW\n",
    "            h = (h) / HH\n",
    "            if flip:\n",
    "                x0 = 1 - (x0 + w)\n",
    "            boxes.append(np.array([x0, y0, w, h]))\n",
    "\n",
    "\n",
    "        # add 0 for number of objects\n",
    "        for _ in range(len(objs), self.max_objects_per_image):\n",
    "            objs.append(self.vocab['object_name_to_idx']['__image__'])\n",
    "            boxes.append(np.array([-0.6, -0.6, 0.5, 0.5]))\n",
    "  \n",
    "        objs = torch.LongTensor(objs)\n",
    "        boxes = np.vstack(boxes)\n",
    "\n",
    "        # triples = torch.LongTensor(triples)\n",
    "        return image, objs, boxes## , b_map #, None # obj_masks #, obj_masks # , b_map # masks # , triples\n",
    "\n",
    "    def get_bbox_map_p(self, bbox):\n",
    "        mapping = np.zeros((len(bbox), self.image_size[0], self.image_size[0]))\n",
    "        for idx in range(self.max_objects_per_image):\n",
    "            if min(bbox[idx]) < 0:\n",
    "                continue\n",
    "            line_space = np.linspace(0, self.image_size[0]-1, num=self.image_size[0])\n",
    "            xv, yv = np.meshgrid(line_space, line_space)\n",
    "            mapping[idx][(xv < int((bbox[idx][0] + bbox[idx][2]) * self.image_size[0])) * (xv > int(bbox[idx][0] * self.image_size[0])) *\n",
    "                    (yv < int((bbox[idx][1] + bbox[idx][3]) * self.image_size[0])) * (yv > int(bbox[idx][1] * self.image_size[0]))] = 1\n",
    "        return mapping\n",
    "\n",
    "\n",
    "def seg_to_mask(seg, width=1.0, height=1.0):\n",
    "    \"\"\"\n",
    "    Tiny utility for decoding segmentation masks using the pycocotools API.\n",
    "    \"\"\"\n",
    "    if type(seg) == list:\n",
    "        rles = mask_utils.frPyObjects(seg, height, width)\n",
    "        rle = mask_utils.merge(rles)\n",
    "    elif type(seg['counts']) == list:\n",
    "        rle = mask_utils.frPyObjects(seg, height, width)\n",
    "    else:\n",
    "        rle = seg\n",
    "    return mask_utils.decode(rle)\n",
    "\n",
    "\n",
    "def coco_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function to be used when wrapping CocoSceneGraphDataset in a\n",
    "    DataLoader. Returns a tuple of the following:\n",
    "  \n",
    "    - imgs: FloatTensor of shape (N, C, H, W)\n",
    "    - objs: LongTensor of shape (O,) giving object categories\n",
    "    - boxes: FloatTensor of shape (O, 4)\n",
    "    - masks: FloatTensor of shape (O, M, M)\n",
    "    - triples: LongTensor of shape (T, 3) giving triples\n",
    "    - obj_to_img: LongTensor of shape (O,) mapping objects to images\n",
    "    - triple_to_img: LongTensor of shape (T,) mapping triples to images\n",
    "    \"\"\"\n",
    "    all_imgs, all_objs, all_boxes, all_masks, all_triples = [], [], [], [], []\n",
    "    all_obj_to_img, all_triple_to_img = [], []\n",
    "    obj_offset = 0\n",
    "    for i, (img, objs, boxes, masks, triples) in enumerate(batch):\n",
    "        all_imgs.append(img[None])\n",
    "        if objs.dim() == 0 or triples.dim() == 0:\n",
    "            continue\n",
    "        O, T = objs.size(0), triples.size(0)\n",
    "        all_objs.append(objs)\n",
    "        all_boxes.append(boxes)\n",
    "        all_masks.append(masks)\n",
    "        triples = triples.clone()\n",
    "        triples[:, 0] += obj_offset\n",
    "        triples[:, 2] += obj_offset\n",
    "        all_triples.append(triples)\n",
    "\n",
    "        all_obj_to_img.append(torch.LongTensor(O).fill_(i))\n",
    "        all_triple_to_img.append(torch.LongTensor(T).fill_(i))\n",
    "        obj_offset += O\n",
    "\n",
    "    all_imgs = torch.cat(all_imgs)\n",
    "    all_objs = torch.cat(all_objs)\n",
    "    all_boxes = torch.cat(all_boxes)\n",
    "    all_masks = torch.cat(all_masks)\n",
    "    all_triples = torch.cat(all_triples)\n",
    "    all_obj_to_img = torch.cat(all_obj_to_img)\n",
    "    all_triple_to_img = torch.cat(all_triple_to_img)\n",
    "\n",
    "    out = (all_imgs, all_objs, all_boxes, all_masks, all_triples,\n",
    "           all_obj_to_img, all_triple_to_img)\n",
    "    return out\n",
    "\n",
    "\n",
    "# IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "# IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "IMAGENET_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGENET_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "INV_IMAGENET_MEAN = [-m for m in IMAGENET_MEAN]\n",
    "INV_IMAGENET_STD = [1.0 / s for s in IMAGENET_STD]\n",
    "\n",
    "\n",
    "def imagenet_preprocess():\n",
    "    return T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "\n",
    "def rescale(x):\n",
    "    lo, hi = x.min(), x.max()\n",
    "    return x.sub(lo).div(hi - lo)\n",
    "\n",
    "\n",
    "def imagenet_deprocess(rescale_image=True):\n",
    "    transforms = [\n",
    "        T.Normalize(mean=[0, 0, 0], std=INV_IMAGENET_STD),\n",
    "        T.Normalize(mean=INV_IMAGENET_MEAN, std=[1.0, 1.0, 1.0]),\n",
    "    ]\n",
    "    if rescale_image:\n",
    "        transforms.append(rescale)\n",
    "    return T.Compose(transforms)\n",
    "\n",
    "def imagenet_deprocess_orig(imgs, rescale=True):\n",
    "    deprocess_fn = imagenet_deprocess(rescale_image=rescale)\n",
    "    imgs = imgs.cpu().clone()\n",
    "    imgs_de = []\n",
    "    for i in range(imgs.size(0)):\n",
    "        img_de = deprocess_fn(imgs[i])[None]\n",
    "        img_de = img_de.mul(255).clamp(0, 255).byte()\n",
    "        imgs_de.append(img_de)\n",
    "    \n",
    "    imgs_de = torch.cat(imgs_de, dim=0)\n",
    "    return imgs_de\n",
    "\n",
    "\n",
    "def imagenet_deprocess_batch(imgs, rescale=True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - imgs: FloatTensor of shape (N, C, H, W) giving preprocessed images\n",
    "    \n",
    "    Output:\n",
    "    - imgs_de: ByteTensor of shape (N, C, H, W) giving deprocessed images\n",
    "    in the range [0, 255]\n",
    "    \"\"\"\n",
    "    if isinstance(imgs, torch.autograd.Variable):\n",
    "        imgs = imgs.data\n",
    "    imgs = imgs.cpu().clone()\n",
    "    deprocess_fn = imagenet_deprocess(rescale_image=rescale)\n",
    "    imgs_de = []\n",
    "    for i in range(imgs.size(0)):\n",
    "        img_de = deprocess_fn(imgs[i])[None]\n",
    "        img_de = img_de.mul(255).clamp(0, 255).byte()\n",
    "        imgs_de.append(img_de)\n",
    "    imgs_de = torch.cat(imgs_de, dim=0)\n",
    "    return imgs_de\n",
    "\n",
    "\n",
    "class Resize(object):\n",
    "    def __init__(self, size, interp=PIL.Image.BILINEAR):\n",
    "        if isinstance(size, tuple):\n",
    "              H, W = size\n",
    "              self.size = (W, H)\n",
    "        else:\n",
    "            self.size = (size, size)\n",
    "        self.interp = interp\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return img.resize(self.size, self.interp)\n",
    "\n",
    "\n",
    "def unpack_var(v):\n",
    "    if isinstance(v, torch.autograd.Variable):\n",
    "        return v.data\n",
    "    return v\n",
    "\n",
    "\n",
    "def split_graph_batch(triples, obj_data, obj_to_img, triple_to_img):\n",
    "    triples = unpack_var(triples)\n",
    "    obj_data = [unpack_var(o) for o in obj_data]\n",
    "    obj_to_img = unpack_var(obj_to_img)\n",
    "    triple_to_img = unpack_var(triple_to_img)\n",
    "\n",
    "    triples_out = []\n",
    "    obj_data_out = [[] for _ in obj_data]\n",
    "    obj_offset = 0\n",
    "    N = obj_to_img.max() + 1\n",
    "    for i in range(N):\n",
    "        o_idxs = (obj_to_img == i).nonzero().view(-1)\n",
    "        t_idxs = (triple_to_img == i).nonzero().view(-1)\n",
    "\n",
    "        cur_triples = triples[t_idxs].clone()\n",
    "        cur_triples[:, 0] -= obj_offset\n",
    "        cur_triples[:, 2] -= obj_offset\n",
    "        triples_out.append(cur_triples)\n",
    "\n",
    "        for j, o_data in enumerate(obj_data):\n",
    "            cur_o_data = None\n",
    "            if o_data is not None:\n",
    "                cur_o_data = o_data[o_idxs]\n",
    "            obj_data_out[j].append(cur_o_data)\n",
    "\n",
    "        obj_offset += o_idxs.size(0)\n",
    "\n",
    "        return triples_out, obj_data_out\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
